{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense,Flatten,Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Re-size all images to standard size used in VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224,224]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Provide the dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'C:/Users/sameer/Downloads/Datasets/Datasets/Train'\n",
    "test_path = 'C:/Users/sameer/Downloads/Datasets/Datasets/Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 11s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vgg = VGG16(input_shape=IMAGE_SIZE+[3], weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Don't train the existing weights. We are using the pretrained model of VGG which was already trained on Imagenet dataset so we will use those weights only. We wont re train those weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1f7bb860608>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbc13748>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bba8f288>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f7bbd83f88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bb868248>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbd8f248>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f7bbd9c0c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbf50bc8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbf541c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbf5d288>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f7bbf60e48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbf5f808>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbf6a2c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbf70f48>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f7bbf75a88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbf7d508>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbf7de88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7bbf87b88>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f7bbf8d648>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.layers   #shows 16 layers objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) As we know we remove bottom and top layer of VGG using include_top=false since it was trained for 1000 categories nd we have just 3. So use glob to state our 3 categories and attach it to the pretraine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders=glob('C:/Users/sameer/Downloads/Datasets/Datasets/Train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/sameer/Downloads/Datasets/Datasets/Train\\\\audi',\n",
       " 'C:/Users/sameer/Downloads/Datasets/Datasets/Train\\\\lamborghini',\n",
       " 'C:/Users/sameer/Downloads/Datasets/Datasets/Train\\\\mercedes']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'block5_pool/MaxPool:0' shape=(None, 7, 7, 512) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Now we have the flatten the vgg output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Flatten()(vgg.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Now put this flattened array to the dense network and then create the model object where inputs and outputs will be two parameters..outputs=predictions and inputs =vgg.input(imagesize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(len(folders),activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model object\n",
    "\n",
    "model = Model(inputs=vgg.input,outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 75267     \n",
      "=================================================================\n",
      "Total params: 14,789,955\n",
      "Trainable params: 75,267\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()   #you can see now we have added 3 nodes as the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Tell the model what optimization and loss u want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Now use image data generator to import the images from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,shear_range = 0.2, zoom_range=0.2, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set=train_datagen.flow_from_directory('C:/Users/sameer/Downloads/Datasets/Datasets/Train',target_size=(224,224),batch_size=32,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set=test_datagen.flow_from_directory('C:/Users/sameer/Downloads/Datasets/Datasets/Test',target_size=(224,224),batch_size=32,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 47s 24s/step - loss: 1.8554 - accuracy: 0.3125 - val_loss: 1.9971 - val_accuracy: 0.3276\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 48s 24s/step - loss: 1.5355 - accuracy: 0.4219 - val_loss: 2.1005 - val_accuracy: 0.1724\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 52s 26s/step - loss: 1.2570 - accuracy: 0.5312 - val_loss: 1.3137 - val_accuracy: 0.5517\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 43s 21s/step - loss: 0.7921 - accuracy: 0.7188 - val_loss: 1.1184 - val_accuracy: 0.6034\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 40s 20s/step - loss: 0.5787 - accuracy: 0.7812 - val_loss: 0.8051 - val_accuracy: 0.6897\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 39s 19s/step - loss: 0.6552 - accuracy: 0.7188 - val_loss: 0.7725 - val_accuracy: 0.5862\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 38s 19s/step - loss: 0.3286 - accuracy: 0.8281 - val_loss: 0.5840 - val_accuracy: 0.7931\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 39s 19s/step - loss: 0.2418 - accuracy: 0.9219 - val_loss: 0.8359 - val_accuracy: 0.6897\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 38s 19s/step - loss: 0.2495 - accuracy: 0.8750 - val_loss: 0.7521 - val_accuracy: 0.7241\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 36s 18s/step - loss: 0.1828 - accuracy: 0.9375 - val_loss: 0.5848 - val_accuracy: 0.7241\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 42s 21s/step - loss: 0.0848 - accuracy: 0.9844 - val_loss: 0.5325 - val_accuracy: 0.8103\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 36s 18s/step - loss: 0.0973 - accuracy: 0.9844 - val_loss: 0.5464 - val_accuracy: 0.7931\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.1273 - accuracy: 0.9688 - val_loss: 0.5288 - val_accuracy: 0.7931\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 32s 16s/step - loss: 0.1009 - accuracy: 0.9531 - val_loss: 0.4698 - val_accuracy: 0.8448\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 31s 15s/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.8276\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 31s 15s/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.5061 - val_accuracy: 0.8276\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 36s 18s/step - loss: 0.0559 - accuracy: 0.9844 - val_loss: 0.5054 - val_accuracy: 0.8276\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 33s 17s/step - loss: 0.0512 - accuracy: 0.9844 - val_loss: 0.4514 - val_accuracy: 0.8448\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 31s 16s/step - loss: 0.0615 - accuracy: 0.9844 - val_loss: 0.3867 - val_accuracy: 0.8448\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 33s 17s/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 0.8793\n"
     ]
    }
   ],
   "source": [
    "r=model.fit(training_set, validation_data=test_set,epochs=20,steps_per_epoch=len(training_set),validation_steps=len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.38500166e-01, 3.29882751e-04, 6.11699745e-02],\n",
       "       [4.41167923e-03, 8.47990155e-01, 1.47598103e-01],\n",
       "       [1.70403630e-01, 3.45090270e-01, 4.84506100e-01],\n",
       "       [4.47861930e-05, 9.96966422e-01, 2.98872613e-03],\n",
       "       [8.01068381e-04, 9.97657180e-01, 1.54163281e-03],\n",
       "       [2.63235602e-03, 9.58827496e-01, 3.85402031e-02],\n",
       "       [6.45751506e-03, 9.75249231e-01, 1.82932690e-02],\n",
       "       [7.64019907e-01, 4.94617643e-03, 2.31033906e-01],\n",
       "       [6.11934721e-01, 2.71673203e-01, 1.16392098e-01],\n",
       "       [9.87760782e-01, 7.72445404e-04, 1.14667462e-02],\n",
       "       [4.34182817e-04, 9.94072974e-01, 5.49282134e-03],\n",
       "       [3.79994838e-03, 8.72737586e-01, 1.23462535e-01],\n",
       "       [4.71872569e-04, 9.98212934e-01, 1.31523365e-03],\n",
       "       [2.67204084e-02, 3.22972797e-02, 9.40982342e-01],\n",
       "       [2.04627011e-02, 1.13415845e-01, 8.66121471e-01],\n",
       "       [1.59044731e-02, 9.77779552e-03, 9.74317789e-01],\n",
       "       [1.55046672e-01, 7.04308785e-03, 8.37910295e-01],\n",
       "       [3.18483621e-01, 4.59936727e-03, 6.76917076e-01],\n",
       "       [4.79394710e-03, 3.46578099e-03, 9.91740227e-01],\n",
       "       [3.86747939e-04, 9.96927321e-01, 2.68592359e-03],\n",
       "       [3.93097335e-03, 9.94394541e-01, 1.67449587e-03],\n",
       "       [1.43660803e-03, 9.66747046e-01, 3.18163149e-02],\n",
       "       [9.28883906e-03, 5.11831045e-01, 4.78880137e-01],\n",
       "       [1.32926635e-03, 9.94026780e-01, 4.64385608e-03],\n",
       "       [3.11523420e-03, 8.19382787e-01, 1.77501947e-01],\n",
       "       [1.73468634e-01, 6.68306828e-01, 1.58224538e-01],\n",
       "       [3.27638787e-04, 9.76865649e-01, 2.28066165e-02],\n",
       "       [1.19386823e-04, 9.98742282e-01, 1.13828317e-03],\n",
       "       [1.68304401e-03, 9.98079777e-01, 2.37104832e-04],\n",
       "       [5.08899987e-03, 2.43613264e-03, 9.92474854e-01],\n",
       "       [5.55804523e-04, 9.94382501e-01, 5.06170094e-03],\n",
       "       [2.52670616e-01, 7.60986879e-02, 6.71230674e-01],\n",
       "       [1.04550915e-02, 9.45114553e-01, 4.44302969e-02],\n",
       "       [5.00310473e-02, 1.95032522e-01, 7.54936397e-01],\n",
       "       [1.50478929e-02, 1.68803796e-01, 8.16148281e-01],\n",
       "       [9.99913454e-01, 5.27936072e-06, 8.12368889e-05],\n",
       "       [7.57959532e-03, 6.04568422e-01, 3.87851983e-01],\n",
       "       [5.79471052e-01, 1.75517909e-02, 4.02977228e-01],\n",
       "       [5.91287687e-02, 7.52751976e-02, 8.65596056e-01],\n",
       "       [2.07727775e-03, 9.83552158e-01, 1.43706026e-02],\n",
       "       [9.14061219e-02, 4.80857998e-01, 4.27735925e-01],\n",
       "       [9.26988723e-04, 9.98497605e-01, 5.75409387e-04],\n",
       "       [7.82274187e-01, 1.77986287e-02, 1.99927211e-01],\n",
       "       [5.33203669e-02, 3.51417005e-01, 5.95262587e-01],\n",
       "       [3.33046634e-03, 8.92219663e-01, 1.04449905e-01],\n",
       "       [1.06873205e-02, 7.74050772e-01, 2.15261877e-01],\n",
       "       [1.98722119e-03, 9.91913080e-01, 6.09968556e-03],\n",
       "       [1.29785109e-03, 7.49503255e-01, 2.49198914e-01],\n",
       "       [2.46463791e-01, 4.34243605e-02, 7.10111856e-01],\n",
       "       [9.38995741e-03, 2.88556307e-03, 9.87724543e-01],\n",
       "       [6.58178749e-03, 9.82060671e-01, 1.13575310e-02],\n",
       "       [3.70267510e-01, 3.39908781e-03, 6.26333475e-01],\n",
       "       [7.84560740e-02, 7.10995914e-03, 9.14433897e-01],\n",
       "       [1.04763899e-02, 7.83242464e-01, 2.06281200e-01],\n",
       "       [3.33335288e-02, 8.94823298e-02, 8.77184093e-01],\n",
       "       [5.52279549e-03, 2.91821867e-01, 7.02655435e-01],\n",
       "       [1.06176034e-04, 9.86021817e-01, 1.38720684e-02],\n",
       "       [1.67640435e-04, 9.95133102e-01, 4.69922973e-03]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred   #3 values are the probability of being audi, lamborhini or mercedes..alphabet wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply argmax and take that index which has the highest value for that record\n",
    "\n",
    "y_pred=np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 2, 1, 1, 1, 0, 2,\n",
       "       1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred   #output of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading image as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.load_img('C:/Users/sameer/Downloads/Datasets/Datasets/Train/audi/1.jpg',target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[233., 219., 208.],\n",
       "        [234., 220., 209.],\n",
       "        [237., 223., 212.],\n",
       "        ...,\n",
       "        [213., 216., 221.],\n",
       "        [218., 221., 226.],\n",
       "        [161., 164., 169.]],\n",
       "\n",
       "       [[240., 226., 215.],\n",
       "        [240., 226., 215.],\n",
       "        [243., 229., 218.],\n",
       "        ...,\n",
       "        [213., 216., 221.],\n",
       "        [217., 220., 225.],\n",
       "        [161., 164., 169.]],\n",
       "\n",
       "       [[247., 233., 222.],\n",
       "        [247., 233., 222.],\n",
       "        [250., 236., 225.],\n",
       "        ...,\n",
       "        [213., 216., 221.],\n",
       "        [217., 220., 225.],\n",
       "        [161., 164., 169.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[130., 112., 108.],\n",
       "        [135., 117., 113.],\n",
       "        [138., 124., 123.],\n",
       "        ...,\n",
       "        [213., 212., 208.],\n",
       "        [215., 210., 207.],\n",
       "        [215., 210., 207.]],\n",
       "\n",
       "       [[126., 108., 104.],\n",
       "        [133., 115., 111.],\n",
       "        [130., 116., 115.],\n",
       "        ...,\n",
       "        [217., 216., 212.],\n",
       "        [218., 213., 210.],\n",
       "        [218., 213., 210.]],\n",
       "\n",
       "       [[121., 103.,  99.],\n",
       "        [133., 115., 111.],\n",
       "        [135., 121., 120.],\n",
       "        ...,\n",
       "        [220., 219., 215.],\n",
       "        [220., 215., 212.],\n",
       "        [220., 215., 212.]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=image.img_to_array(img)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00358324, 0.00336794, 0.00319877],\n",
       "        [0.00359862, 0.00338331, 0.00321415],\n",
       "        [0.00364475, 0.00342945, 0.00326028],\n",
       "        ...,\n",
       "        [0.00327566, 0.0033218 , 0.00339869],\n",
       "        [0.00335256, 0.00339869, 0.00347559],\n",
       "        [0.00247597, 0.00252211, 0.002599  ]],\n",
       "\n",
       "       [[0.00369089, 0.00347559, 0.00330642],\n",
       "        [0.00369089, 0.00347559, 0.00330642],\n",
       "        [0.00373702, 0.00352172, 0.00335256],\n",
       "        ...,\n",
       "        [0.00327566, 0.0033218 , 0.00339869],\n",
       "        [0.00333718, 0.00338331, 0.00346021],\n",
       "        [0.00247597, 0.00252211, 0.002599  ]],\n",
       "\n",
       "       [[0.00379854, 0.00358324, 0.00341407],\n",
       "        [0.00379854, 0.00358324, 0.00341407],\n",
       "        [0.00384468, 0.00362937, 0.00346021],\n",
       "        ...,\n",
       "        [0.00327566, 0.0033218 , 0.00339869],\n",
       "        [0.00333718, 0.00338331, 0.00346021],\n",
       "        [0.00247597, 0.00252211, 0.002599  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.00199923, 0.00172241, 0.0016609 ],\n",
       "        [0.00207612, 0.00179931, 0.00173779],\n",
       "        [0.00212226, 0.00190696, 0.00189158],\n",
       "        ...,\n",
       "        [0.00327566, 0.00326028, 0.00319877],\n",
       "        [0.00330642, 0.00322953, 0.00318339],\n",
       "        [0.00330642, 0.00322953, 0.00318339]],\n",
       "\n",
       "       [[0.00193772, 0.0016609 , 0.00159938],\n",
       "        [0.00204537, 0.00176855, 0.00170704],\n",
       "        [0.00199923, 0.00178393, 0.00176855],\n",
       "        ...,\n",
       "        [0.00333718, 0.0033218 , 0.00326028],\n",
       "        [0.00335256, 0.00327566, 0.00322953],\n",
       "        [0.00335256, 0.00327566, 0.00322953]],\n",
       "\n",
       "       [[0.00186082, 0.00158401, 0.00152249],\n",
       "        [0.00204537, 0.00176855, 0.00170704],\n",
       "        [0.00207612, 0.00186082, 0.00184544],\n",
       "        ...,\n",
       "        [0.00338331, 0.00336794, 0.00330642],\n",
       "        [0.00338331, 0.00330642, 0.00326028],\n",
       "        [0.00338331, 0.00330642, 0.00326028]]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=x/255\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.expand_dims(x,axis=0)\n",
    "img_data=preprocess_input(x)\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.7535434e-05, 1.1717037e-06, 9.9998128e-01]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.argmax(model.predict(img_data),axis=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
